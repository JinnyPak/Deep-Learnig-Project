{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver import ChromeOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_2016=pd.read_csv('ks_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#크롤링 block 당했거나 오류가 났을 때, \n",
    "#continue하고 다음 url로 바로 넘어가는게 아니라 약 2분 정도 간격으로 10번까지 접속시도 \n",
    "#접속되면 scrape하고 안 되면 continue되도록 한다\n",
    "options = ChromeOptions()\n",
    "\n",
    "driver = Chrome(options=options)\n",
    "content_list = []\n",
    "for idx, url in enumerate( ks_2016['url_project'][7000:8000], start=7000):\n",
    "    driver.get(url)\n",
    "    time.sleep(random.uniform(8,20))\n",
    "    req = driver.page_source\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    try:\n",
    "        content_tag = soup.select_one('div.rte__content')\n",
    "        contents = content_tag.select('p')\n",
    "        contents_collected = []\n",
    "        for c in contents:\n",
    "            content = c.get_text().strip()\n",
    "            contents_collected.append(content)\n",
    "        try:\n",
    "            risk_challenge_tag = soup.select_one('div#risksAndChallenges')\n",
    "            risk_challenge_list = risk_challenge_tag.select('p.js-risks-text.text-preline')\n",
    "            for rc in risk_challenge_list:\n",
    "                risk_challenge = rc.get_text().strip()\n",
    "        except:\n",
    "            risk_challenge=\" \" #또는 np.nan\n",
    "        #오류 나기 전까지 저장되어 있는 content_list에는 이어서 append\n",
    "        content_list.append([idx, contents_collected, risk_challenge])\n",
    "        #오류 난 후부턴 다시 크롤링 할 때, 매 페이지 긁어온 것마다 바로바로 csv파일에 한줄씩 append 할 것이다\n",
    "        content_list_since_error = []\n",
    "        content_list_since_error.append([idx, contents_collected, risk_challenge])\n",
    "        pd.DataFrame(content_list_since_error, columns=['index','content','risk_challenge']).to_csv('ks_content_crawled_2016_7.csv', index=False, header=False, mode='a', encoding='UTF-8')\n",
    "        print(idx)\n",
    "    except Exception as ex:\n",
    "        print(\"Unexpected error at {}\".format(idx), ex)\n",
    "        error_url = url\n",
    "        count = 0\n",
    "        while count < 11:\n",
    "            print(\"Retry:\", count)\n",
    "            driver.get(error_url)\n",
    "            time.sleep(random.uniform(8,20))\n",
    "            req = driver.page_source\n",
    "            soup = BeautifulSoup(req, 'lxml')\n",
    "            try:\n",
    "                content_tag = soup.select_one('div.rte__content')\n",
    "                contents = content_tag.select('p')\n",
    "                contents_collected = []\n",
    "                for c in contents:\n",
    "                    content = c.get_text().strip()\n",
    "                    contents_collected.append(content)\n",
    "                try:\n",
    "                    risk_challenge_tag = soup.select_one('div#risksAndChallenges')\n",
    "                    risk_challenge_list = risk_challenge_tag.select('p.js-risks-text.text-preline')\n",
    "                    for rc in risk_challenge_list:\n",
    "                        risk_challenge = rc.get_text().strip()\n",
    "                except:\n",
    "                    risk_challenge=\" \" #또는 np.nan\n",
    "                content_list.append([idx, contents_collected, risk_challenge])\n",
    "                content_list_since_error = []\n",
    "                content_list_since_error.append([idx, contents_collected, risk_challenge])\n",
    "                pd.DataFrame(content_list_since_error, columns=['index','content','risk_challenge']).to_csv('ks_content_crawled_2016_7.csv', index=False, header=False, mode='a', encoding='UTF-8')\n",
    "                print(idx)\n",
    "                break\n",
    "            except Exception as et:\n",
    "                print(\"Error while retrying:\", et)\n",
    "                time.sleep(random.uniform(100,130))\n",
    "                count += 1\n",
    "                continue\n",
    "        #만약 재시도를 10번 했는데도 크롤링에 실패시 \n",
    "        #오류 index, url, nan값을 csv파일들에 저장\n",
    "        if count == 11:\n",
    "            error_list = []\n",
    "            error_list.append([idx, error_url])\n",
    "            pd.DataFrame(error_list, columns=['index','error_url']).to_csv('ks_middle_errors_2016_7.csv', index=False, header=False, mode='a', encoding='UTF-8')\n",
    "            content_list.append([idx, error_url, np.nan])\n",
    "            error_into_saved_csv = []\n",
    "            error_into_saved_csv.append([idx, error_url, np.nan])\n",
    "            pd.DataFrame(error_into_saved_csv, columns=['index','content','risk_challenge']).to_csv('ks_content_crawled_2016_7.csv', index=False, header=False, mode='a', encoding='UTF-8')\n",
    "        continue\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
