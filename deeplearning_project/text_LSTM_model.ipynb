{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset=pd.read_csv('text_dataset_2015_2019.csv')\n",
    "meta_2015_2019=pd.read_csv('meta_dataset_2015_2019.csv')\n",
    "ks_text_state=pd.concat([text_dataset[['content','risk_challenge']],meta_2015_2019['state']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결측치확인\n",
    "ks_text_state.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null인 content 행 걸러내기\n",
    "null_index=ks_text_state[ks_text_state.content.isna()].index.tolist()#null값있는 행 index\n",
    "ks_text_state=ks_text_state.drop(null_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#공백이 있는 content([]) 걸러내기\n",
    "cont_null_index=[]\n",
    "for i in ks_text_state.index:\n",
    "    if ks_text_state.content.loc[i]==\"[]\":\n",
    "        cont_null_index.append(i)\n",
    "ks_text_state=ks_text_state.drop(cont_null_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null인 risk_challenge 확인하고 \" \"으로 대체\n",
    "risk_isna=ks_text_state[ks_text_state.risk_challenge.isna()].index.tolist()\n",
    "ks_text_state.risk_challenge=ks_text_state.risk_challenge.fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제거한 행의 index list->다른 데이터들과 맞추기위해 남겨두기\n",
    "total_null_index=null_index+cont_null_index\n",
    "\n",
    "#null값인 행의 index 목록 pickle 저장\n",
    "import pickle\n",
    "with open('total_null_index2.pkl','wb') as f:\n",
    "    pickle.dump(total_null_index,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_text_state['all_text']=ks_text_state['content']+ks_text_state['risk_challenge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=ks_text_state['state']\n",
    "X=ks_text_state['all_text']\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y label encoding\n",
    "#라벨링\n",
    "#Series를 받아서 라벨인코딩 처리하는 함수\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "col_dict={}\n",
    "\n",
    "def encoding(x):\n",
    "    le=LabelEncoder()\n",
    "#fit()어떻게 변환할지 fitting\n",
    "    le.fit(x)\n",
    "#변환\n",
    "    label_x=le.transform(x)\n",
    "    col_dict[x.name]=le.classes_\n",
    "    return label_x\n",
    "y_enc=encoding(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "# 전처리 함수\n",
    "def text_preprocessing(document):\n",
    "#     if math.isnan(document):\n",
    "#         return\n",
    "    \n",
    "    # 소문자 변환\n",
    "    type(document)\n",
    "    document = document.lower()\n",
    "    document = document.replace('\\\\xa0',' ')\n",
    "    document = document.replace('•\\\\t',' ') \n",
    "    # 특수문자 제거\n",
    "    pattern = '[{}]'.format(string.punctuation)\n",
    "    document = re.sub(pattern, ' ', document)\n",
    "    # stopword 제거, stemming\n",
    "    sw = stopwords.words('english')+['may']\n",
    "    word_token=nltk.word_tokenize(document)\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    result_token=[ stemmer.stem(word) for word in word_token if word not in sw]\n",
    "    #문자열로 변환 후 반환\n",
    "    return ' '.join(result_token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list=list(X.values)\n",
    "text = [text_preprocessing(x) for x in X_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리한 텍스트데이터 pickle저장\n",
    "with open('preprocessed_all_text.pkl','wb') as f:\n",
    "    pickle.dump(text,f)\n",
    "with open('y_encoding.pkl','wb') as f:\n",
    "    pickle.dump(y_enc,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #읽기\n",
    "# import pickle\n",
    "# with open('y_encoding.pkl','rb') as f:\n",
    "#     y_enc=pickle.load(f)\n",
    "# with open('preprocessed_all_text.pkl','rb') as f:\n",
    "#     preprocessed_all_text=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=10000\n",
    "maxlen=500\n",
    "batch_size=32\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(preprocessed_all_text)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('단어 집합의 크기 : %d' % vocab_size)\n",
    "\n",
    "sequences=tokenizer.texts_to_sequences(preprocessed_all_text)\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "data=pad_sequences(sequences,maxlen=maxlen)\n",
    "labels=np.array(y_enc)\n",
    "print(data.shape,labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels,stratify=labels) \n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense,LSTM,SimpleRNN,Dropout\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy','AUC','Recall','Precision'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,validation_split=0.2)\n",
    "model.save('lstm_all_text.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 저장한 모델 로드하여 예측하기\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "new_model = load_model('lstm_all_text.h5')\n",
    "\n",
    "test1_pre = [text_preprocessing(x) for x in test1]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(test1_pre)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "sequences=tokenizer.texts_to_sequences(test1_pre)\n",
    "data=pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "pred_cls = new_model.predict_classes(data)\n",
    "\n",
    "pred_proba=new_model.predict_proba(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
